{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43eb58cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data loaded successfully.\n",
      "X_train shape: (914476, 95), y_train shape: (914476,)\n",
      "X_test shape: (43109, 95), y_test shape: (43109,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Setup and Data Loading ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb # Import LightGBM\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output, if desired\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Project root setup\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parent.parent\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path('.').resolve().parent\n",
    "\n",
    "# Load processed data\n",
    "try:\n",
    "    X_train = pd.read_csv(PROJECT_ROOT / \"data/processed/X_train_processed.csv\")\n",
    "    y_train = pd.read_csv(PROJECT_ROOT / \"data/processed/y_train_processed.csv\").squeeze()\n",
    "    X_test = pd.read_csv(PROJECT_ROOT / \"data/processed/X_test_processed.csv\")\n",
    "    y_test = pd.read_csv(PROJECT_ROOT / \"data/processed/y_test_processed.csv\").squeeze()\n",
    "    print(\"Processed data loaded successfully.\")\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading processed data: {e}. Make sure 04_tree_model.ipynb was run and saved the data.\")\n",
    "    exit() # Exit if data isn't found, as further steps will fail\n",
    "\n",
    "# Helper Function for Categorical Reconstruction (from 05_error_analysis)\n",
    "def reconstruct_category(df, prefix):\n",
    "    \"\"\"\n",
    "    Reconstructs a single categorical column from its one-hot encoded dummy columns.\n",
    "    Example: If df has 'town_bishan', 'town_bedok', etc., and prefix is 'town_',\n",
    "    it will create a 'town' column with values 'bishan', 'bedok'.\n",
    "    \"\"\"\n",
    "    category_dummy_cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "    if not category_dummy_cols:\n",
    "        return pd.Series(np.nan, index=df.index, name=prefix.rstrip('_'))\n",
    "\n",
    "    reconstructed_series = df[category_dummy_cols].idxmax(axis=1).str.replace(prefix, '')\n",
    "    return reconstructed_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a00377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Random Forest Regressor ---\n",
      "Random Forest MAE: 59,963 SGD\n",
      "Random Forest RMSE: 79,312 SGD\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Model 1: Random Forest Regressor ---\n",
    "print(\"\\n--- Training Random Forest Regressor ---\")\n",
    "rf_r = RandomForestRegressor(\n",
    "    n_estimators=100, # Using 100 as per previous setup\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "rf_r.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_preds = rf_r.predict(X_test)\n",
    "rf_mae = mean_absolute_error(y_test, rf_preds)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_preds))\n",
    "\n",
    "print(f\"Random Forest MAE: {rf_mae:,.0f} SGD\")\n",
    "print(f\"Random Forest RMSE: {rf_rmse:,.0f} SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be13554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned higher weights to 69467 rare class flats.\n"
     ]
    }
   ],
   "source": [
    "# --- Create Sample Weights for Rare Classes ---\n",
    "# Initialize all weights to 1\n",
    "sample_weights = np.ones(len(X_train))\n",
    "\n",
    "# Identify Executive and Multi-Generation flats in the training set\n",
    "# These are the one-hot encoded columns in X_train\n",
    "executive_mask = X_train['flat_type_executive'] == True\n",
    "multi_gen_mask = X_train['flat_type_multi_generation'] == True\n",
    "\n",
    "# Assign a higher weight (e.g., 2 times the normal weight)\n",
    "sample_weights[executive_mask] = 2.0\n",
    "sample_weights[multi_gen_mask] = 2.0\n",
    "\n",
    "print(f\"Assigned higher weights to {executive_mask.sum() + multi_gen_mask.sum()} rare class flats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f8654a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LightGBM Regressor ---\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 560\n",
      "[LightGBM] [Info] Number of data points in the train set: 914476, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 330971.520749\n",
      "LightGBM MAE: 53,886 SGD\n",
      "LightGBM RMSE: 68,322 SGD\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Model 2: LightGBM Regressor ---\n",
    "print(\"\\n--- Training LightGBM Regressor ---\")\n",
    "lgbm_r = lgb.LGBMRegressor(\n",
    "    n_estimators=10000, # Since LightGBM is optimized for speed and performance\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31, # Default value, can be tuned\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    colsample_bytree=0.7, # Feature subsampling\n",
    "    subsample=0.7,        # Data subsampling\n",
    "    reg_alpha=0.1,        # L1 regularization\n",
    "    reg_lambda=0.1,        # L2 regularization\n",
    ")\n",
    "lgbm_r.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Evaluate LightGBM\n",
    "lgbm_preds = lgbm_r.predict(X_test)\n",
    "lgbm_mae = mean_absolute_error(y_test, lgbm_preds)\n",
    "lgbm_rmse = np.sqrt(mean_squared_error(y_test, lgbm_preds))\n",
    "\n",
    "print(f\"LightGBM MAE: {lgbm_mae:,.0f} SGD\")\n",
    "print(f\"LightGBM RMSE: {lgbm_rmse:,.0f} SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a35eb4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Comparison Summary ---\n",
      "Random Forest MAE: 59,963 SGD\n",
      "LightGBM MAE:    53,886 SGD\n",
      "\n",
      "LightGBM performed better by 6,077 SGD MAE!\n",
      "\n",
      "Proceeding with error analysis for the LightGBM model.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Compare Models (Initial Metrics) ---\n",
    "print(\"\\n--- Model Comparison Summary ---\")\n",
    "print(f\"Random Forest MAE: {rf_mae:,.0f} SGD\")\n",
    "print(f\"LightGBM MAE:    {lgbm_mae:,.0f} SGD\")\n",
    "\n",
    "if lgbm_mae < rf_mae:\n",
    "    print(f\"\\nLightGBM performed better by {rf_mae - lgbm_mae:,.0f} SGD MAE!\")\n",
    "    best_model_name = \"LightGBM\"\n",
    "    best_model = lgbm_r\n",
    "    best_preds = lgbm_preds\n",
    "    best_mae = lgbm_mae\n",
    "    best_rmse = lgbm_rmse # <--- ADDED THIS LINE\n",
    "else:\n",
    "    print(f\"\\nRandom Forest performed better by {lgbm_mae - rf_mae:,.0f} SGD MAE!\")\n",
    "    best_model_name = \"Random Forest\"\n",
    "    best_model = rf_r\n",
    "    best_preds = rf_preds\n",
    "    best_mae = rf_mae\n",
    "    best_rmse = rf_rmse # <--- ADDED THIS LINE\n",
    "\n",
    "print(f\"\\nProceeding with error analysis for the {best_model_name} model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a038ed4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top-20 worst absolute errors for LightGBM ---\n",
      "          actual     predicted      abs_error  sale_year           town  flat_type  floor_area_sqm  lease_remaining_years\n",
      "17633  1260000.0  7.403382e+05  519661.809437       2024          bedok     5_room           134.0                   53.0\n",
      "39056  1230000.0  7.930923e+05  436907.699049       2025         bishan     4_room           113.0                   63.0\n",
      "36473  1518000.0  1.133697e+06  384303.259399       2025   central_area     4_room            94.0                   85.0\n",
      "37569  1500000.0  1.123010e+06  376990.314804       2025          bedok     5_room           120.0                   85.0\n",
      "34607  1600000.0  1.224054e+06  375945.521822       2025      toa_payoh     5_room           117.0                   86.0\n",
      "31511  1260000.0  8.884794e+05  371520.586001       2025     queenstown     4_room            87.0                   86.0\n",
      "29212  1218000.0  8.612185e+05  356781.468831       2025      serangoon  executive           150.0                   66.0\n",
      "19526  1448000.0  1.091888e+06  356111.607492       2024         bishan  executive           141.0                   67.0\n",
      "4429   1380000.0  1.024507e+06  355493.301003       2024  marine_parade     5_room           157.0                   50.0\n",
      "30870  1350000.0  9.973942e+05  352605.840475       2025     queenstown     5_room           124.0                   72.0\n",
      "35825  1338000.0  9.900976e+05  347902.442108       2025    bukit_merah     5_room           115.0                   79.0\n",
      "35575  1338000.0  1.005137e+06  332863.021739       2025    bukit_merah     4_room            92.0                   90.0\n",
      "31509  1250000.0  9.180574e+05  331942.619983       2025     queenstown     4_room            92.0                   86.0\n",
      "42622  1280000.0  9.550010e+05  324998.980077       2025        hougang  executive           145.0                   61.0\n",
      "1098   1200000.0  8.819876e+05  318012.422444       2024     queenstown     4_room            95.0                   86.0\n",
      "30828  1510000.0  1.192075e+06  317925.472719       2025     queenstown  executive           146.0                   69.0\n",
      "16908  1295000.0  9.777427e+05  317257.257121       2024    bukit_merah     5_room           115.0                   75.0\n",
      "35817  1340000.0  1.024196e+06  315803.579300       2025    bukit_merah     5_room           115.0                   74.0\n",
      "1247   1550000.0  1.234223e+06  315776.626057       2024     queenstown     5_room           124.0                   86.0\n",
      "34930  1260000.0  9.456444e+05  314355.610923       2025      toa_payoh     5_room           110.0                   83.0\n",
      "\n",
      "--- Worst towns by MAE for LightGBM ---\n",
      "town\n",
      "bishan             83844.912499\n",
      "serangoon          77988.499663\n",
      "bukit_timah        73760.547239\n",
      "central_area       71381.646420\n",
      "kallang_whampoa    70550.335738\n",
      "queenstown         67418.179183\n",
      "toa_payoh          66328.430843\n",
      "pasir_ris          63345.245167\n",
      "sembawang          61699.679277\n",
      "sengkang           61243.031650\n",
      "Name: error, dtype: float64\n",
      "\n",
      "--- Worst flat_types by MAE for LightGBM ---\n",
      "flat_type\n",
      "multi_generation    119694.973115\n",
      "executive            71642.912567\n",
      "5_room               60782.725923\n",
      "4_room               57338.211011\n",
      "3_room               40400.855048\n",
      "2_room               25080.545678\n",
      "Name: error, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Error Analysis (for the best performing model) ---\n",
    "\n",
    "# Attach predictions & error to test frame for detailed analysis\n",
    "test_df_analysis = X_test.copy()\n",
    "test_df_analysis[\"actual\"] = y_test.values\n",
    "test_df_analysis[\"predicted\"] = best_preds\n",
    "test_df_analysis[\"abs_error\"] = (test_df_analysis[\"actual\"] - test_df_analysis[\"predicted\"]).abs()\n",
    "\n",
    "# Reconstruct 'town' and 'flat_type' from one-hot encoded columns in test_df_analysis\n",
    "test_df_analysis['town'] = reconstruct_category(test_df_analysis, 'town_')\n",
    "test_df_analysis['flat_type'] = reconstruct_category(test_df_analysis, 'flat_type_')\n",
    "\n",
    "\n",
    "# 5.1.2 Worst 20 rows\n",
    "worst20 = test_df_analysis.sort_values(\"abs_error\", ascending=False).head(20)\n",
    "display_cols = [\"actual\", \"predicted\", \"abs_error\",\n",
    "                \"sale_year\", \"town\", \"flat_type\", \"floor_area_sqm\", \"lease_remaining_years\"]\n",
    "print(f\"\\n--- Top-20 worst absolute errors for {best_model_name} ---\")\n",
    "print(worst20[display_cols].to_string())\n",
    "\n",
    "\n",
    "# 5.1.3 Error slices – town & flat_type\n",
    "def mae_by(group_col, df_to_analyze):\n",
    "    return (df_to_analyze\n",
    "            .assign(error=df_to_analyze[\"abs_error\"])\n",
    "            .groupby(group_col)[\"error\"]\n",
    "            .mean()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(10))\n",
    "\n",
    "print(f\"\\n--- Worst towns by MAE for {best_model_name} ---\")\n",
    "# Ensure 'town' column has no NaNs that might have resulted from reconstruction failures\n",
    "print(mae_by(\"town\", test_df_analysis.dropna(subset=['town'])))\n",
    "\n",
    "print(f\"\\n--- Worst flat_types by MAE for {best_model_name} ---\")\n",
    "# Ensure 'flat_type' column has no NaNs\n",
    "print(mae_by(\"flat_type\", test_df_analysis.dropna(subset=['flat_type'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "868d7222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model (LightGBM) saved successfully.\n",
      "Feature list saved successfully.\n",
      "Model comparison metrics saved.\n",
      "\n",
      "Notebook 06_model_comparison.ipynb execution complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Save Best Model and Metrics ---\n",
    "(PROJECT_ROOT / \"models\").mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(best_model, PROJECT_ROOT / f\"models/{best_model_name.lower().replace(' ', '_')}_comparison_model.joblib\")\n",
    "print(f\"\\nBest model ({best_model_name}) saved successfully.\")\n",
    "\n",
    "feature_list = X_train.columns.tolist()\n",
    "joblib.dump(feature_list, PROJECT_ROOT / \"models/feature_list.joblib\")\n",
    "print(\"Feature list saved successfully.\")\n",
    "\n",
    "(PROJECT_ROOT / \"reports\").mkdir(parents=True, exist_ok=True)\n",
    "metrics_df = pd.DataFrame([{'model': best_model_name, 'MAE': best_mae, 'RMSE': best_rmse}])\n",
    "metrics_df.to_csv(PROJECT_ROOT / \"reports/model_comparison_metrics.csv\", index=False)\n",
    "print(\"Model comparison metrics saved.\")\n",
    "\n",
    "print(\"\\nNotebook 06_model_comparison.ipynb execution complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdb-price-predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

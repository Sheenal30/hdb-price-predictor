{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Phase 4 – Better model (RandomForest) + Explainability\n",
    "# ================================================================\n",
    "# Why RandomForest?\n",
    "# • Handles 300+ dummy features with zero preprocessing\n",
    "# • Robust to multicollinearity and outliers\n",
    "# • Ships with simple .feature_importances_\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import shap   # already in requirements\n",
    "from sklearn.impute import SimpleImputer # Need this for NaN imputation\n",
    "\n",
    "# Load\n",
    "clean_path = Path(\"../data/processed/clean_hdb.csv\")  # adjust if notebook lives elsewhere\n",
    "df = pd.read_csv(clean_path, parse_dates=[\"sale_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe40c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Re-applying Preprocessing from Phase 3\n",
    "# Random Forest still needs numerical inputs, so strings must be handled.\n",
    "# ================================================================\n",
    "\n",
    "# Convert 'sale_date' to datetime if it's not already (important for sorting/feature engineering)\n",
    "# (Already done by parse_dates in read_csv, but good to be explicit if it wasn't)\n",
    "df['sale_date'] = pd.to_datetime(df['sale_date'])\n",
    "\n",
    "# Identify columns to drop (identifiers, redundant string columns, highly cardinal strings)\n",
    "cols_to_drop_pre_encoding = [\n",
    "    '_id',        # Identifier\n",
    "    'month',      # Redundant with 'sale_month' (if exists) and is a string\n",
    "    'town',       # Redundant if town_X columns are already present and used\n",
    "    'flat_type',  # Redundant if flat_type_X columns are already present and used\n",
    "    'block',      # Highly cardinal string, not suitable for direct encoding in RF either (too many categories)\n",
    "    'street_name' # Highly cardinal string, not suitable for direct encoding in RF either\n",
    "]\n",
    "# Drop these columns if they exist in the DataFrame\n",
    "df = df.drop(columns=[col for col in cols_to_drop_pre_encoding if col in df.columns])\n",
    "\n",
    "# Identify remaining object Dtype columns that need one-hot encoding\n",
    "# Based on your df.info(), these are likely 'storey_range' and 'flat_model'\n",
    "categorical_cols_to_encode = [\n",
    "    c for c in df.columns if df[c].dtype == 'object'\n",
    "]\n",
    "\n",
    "# Apply one-hot encoding to the remaining specified categorical columns\n",
    "if categorical_cols_to_encode:\n",
    "    df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
    "\n",
    "# Handle Missing Values (NaN) - 'lease_remaining_years' and 'flat_age'\n",
    "numerical_cols_with_nans = ['lease_remaining_years', 'flat_age']\n",
    "# Ensure these columns exist before trying to impute them\n",
    "numerical_cols_with_nans = [col for col in numerical_cols_with_nans if col in df.columns]\n",
    "\n",
    "if numerical_cols_with_nans: # Only impute if there are actually columns to impute\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df[numerical_cols_with_nans] = imputer.fit_transform(df[numerical_cols_with_nans])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133195d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1  Load the same train / test split we used for linear\n",
    "df = df.sort_values(\"sale_year\")\n",
    "train = df[df.sale_year <= 2023]\n",
    "test  = df[df.sale_year >= 2024]\n",
    "\n",
    "y_col = \"resale_price\"\n",
    "X_cols = [c for c in df.columns if c not in [y_col, \"price_per_sqm\", \"sale_date\"]]\n",
    "\n",
    "X_train, y_train = train[X_cols], train[y_col]\n",
    "X_test , y_test  = test [X_cols], test [y_col]\n",
    "\n",
    "# 4.1.1  Fit a quick RandomForest\n",
    "rf = RandomForestRegressor(\n",
    "        n_estimators=500,  # more trees = stabler importances\n",
    "        max_depth=None,    # let it grow; RF averages out over-fit\n",
    "        n_jobs=-1,\n",
    "        random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 4.1.2  Evaluate\n",
    "preds = rf.predict(X_test)\n",
    "mae   = mean_absolute_error(y_test, preds)\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "rmse = np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2  Compare vs baseline\n",
    "baseline = pd.read_csv(\"reports/baseline_metrics.csv\")\n",
    "improved = pd.DataFrame({\n",
    "    \"model\": [\"RandomForest\"],\n",
    "    \"MAE_SGD\": [round(mae, 1)],\n",
    "    \"RMSE_SGD\": [round(rmse, 1)],\n",
    "})\n",
    "metrics = pd.concat([baseline, improved], ignore_index=True)\n",
    "metrics.to_csv(\"reports/model_metrics.csv\", index=False)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff1f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3.1  Feature-importance bar chart  (top 20)\n",
    "importances = pd.Series(rf.feature_importances_, index=X_cols)\n",
    "top20 = importances.sort_values(ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "top20.sort_values().plot(kind=\"barh\")\n",
    "plt.title(\"RandomForest – top 20 feature importances\")\n",
    "plt.xlabel(\"Gini importance (mean decrease in impurity)\")\n",
    "plt.tight_layout()\n",
    "fig_path = Path(\"reports/figures/rf_importance.png\")\n",
    "fig_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_path, dpi=120)\n",
    "plt.show()\n",
    "\n",
    "# 4.3.2  SHAP summary (optional but quick on a 500-tree RF)\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X_train.sample(5_000, random_state=42))  # subsample for speed\n",
    "\n",
    "shap.summary_plot(shap_values,\n",
    "                  X_train.sample(5_000, random_state=42),\n",
    "                  show=False, max_display=20)\n",
    "shap_path = Path(\"reports/figures/shap_summary.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(shap_path, dpi=120)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPlots saved to:\\n • {fig_path}\\n • {shap_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e453904",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Commentary — what & why (keep in notebook markdown)\n",
    "\t•\tTime-series split preserved – same ≤2023 train / ≥2024 test so numbers are apples-to-apples with Phase 3.\n",
    "\t•\tRandomForest baseline – zero tuning, yet handles dummy-heavy data far better than linear regression.\n",
    "\t•\tResults table now holds both models; MAE drop shows the gain (expect ~20-30 k cut).\n",
    "\t•\tFeature importances – quick Gini-gain bar confirms lease, town dummies, and flat_size drive predictions.\n",
    "\t•\tSHAP summary – nicer, model-agnostic explanation; red = feature pushes price up, blue = down.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdb-price-predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
